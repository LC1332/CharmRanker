"""
æµ‹è¯•å„å®¶MLLM APIçš„ç›®æ ‡æ£€æµ‹èƒ½åŠ›
- å…ˆæµ‹è¯•APIè¿é€šæ€§ï¼ˆæ–‡æœ¬é—®ç­”ï¼‰
- ç„¶åæµ‹è¯•ç›®æ ‡æ£€æµ‹ï¼ˆæ¡†å‡ºéª†é©¼ï¼‰
- å¯è§†åŒ–ç»“æœå¹¶ä¿å­˜
"""

from __future__ import annotations

import os
import sys

# æ¸…é™¤å¯èƒ½å¹²æ‰°çš„ç¯å¢ƒå˜é‡ï¼Œå¿…é¡»åœ¨å¯¼å…¥SDKä¹‹å‰
os.environ.pop("OPENAI_API_KEY", None)
os.environ.pop("GOOGLE_API_KEY", None)

import json
import base64
import requests
from typing import Optional, List, Dict
from pathlib import Path
from dotenv import load_dotenv
from PIL import Image, ImageDraw, ImageFont
from openai import OpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# APIé…ç½®
API_KEY = os.getenv("LUMOS_API")
if not API_KEY:
    print("âŒ é”™è¯¯: è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®LUMOS_API")
    sys.exit(1)


# API base URLs (ä»ç¯å¢ƒå˜é‡è¯»å–)
BASE_URLS = {
    "openai": os.getenv("OPENAI_BASE_URL", ""),
    "gemini": os.getenv("GEMINI_BASE_URL", ""),
    "qwen": os.getenv("QWEN_BASE_URL", ""),
    "claude": os.getenv("CLAUDE_BASE_URL", ""),
}

# æ¨¡å‹é…ç½®
MODELS = {
    "openai": "gpt-4o",
    "gemini": "gemini-3-flash-preview",  # Gemini 3 æ”¯æŒç›®æ ‡æ£€æµ‹
    "qwen": "qwen-vl-max",  # é€šä¹‰åƒé—®è§†è§‰æ¨¡å‹
    "claude": "claude-sonnet-4-20250514",
}

# æµ‹è¯•å›¾ç‰‡è·¯å¾„
TEST_IMAGE = Path(__file__).parent / "data" / "test_luotuo.jpg"
OUTPUT_DIR = Path(__file__).parent / "local_data" / "output"


def gemini_generate_content(model: str, contents: list, response_mime_type: str = None) -> dict:
    """è°ƒç”¨ Gemini REST API"""
    url = f"{BASE_URLS['gemini']}/v1/models/{model}:generateContent"
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    
    payload = {"contents": contents}
    if response_mime_type:
        payload["generationConfig"] = {"responseMimeType": response_mime_type}
    
    response = requests.post(url, headers=headers, json=payload, timeout=120)
    response.raise_for_status()
    return response.json()


def test_text_api(provider: str) -> bool:
    """æµ‹è¯•æ–‡æœ¬APIè¿é€šæ€§"""
    print(f"\nğŸ”„ æµ‹è¯• {provider} æ–‡æœ¬APIè¿é€šæ€§...")
    
    try:
        if provider == "gemini":
            # Gemini ä½¿ç”¨ REST API
            response = gemini_generate_content(
                model=MODELS["gemini"],
                contents=[{"parts": [{"text": "Hello, please respond with 'API connection successful' in Chinese."}], "role": "user"}]
            )
            result = response["candidates"][0]["content"]["parts"][0]["text"]
        else:
            # OpenAIå…¼å®¹æ¥å£
            client = OpenAI(api_key=API_KEY, base_url=BASE_URLS[provider])
            response = client.chat.completions.create(
                model=MODELS[provider],
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": "Hello, please respond with 'API connection successful' in Chinese."},
                ],
                max_tokens=100,
            )
            result = response.choices[0].message.content
        
        print(f"âœ… {provider} è¿é€šæ€§æµ‹è¯•æˆåŠŸ: {result[:50]}...")
        return True
    except Exception as e:
        print(f"âŒ {provider} è¿é€šæ€§æµ‹è¯•å¤±è´¥: {e}")
        return False


def load_image_as_base64(image_path: Path) -> str:
    """åŠ è½½å›¾ç‰‡å¹¶è½¬æ¢ä¸ºbase64"""
    with open(image_path, "rb") as f:
        return base64.b64encode(f.read()).decode("utf-8")


def test_detection_gemini() -> Optional[List[Dict]]:
    """ä½¿ç”¨Geminiè¿›è¡Œç›®æ ‡æ£€æµ‹"""
    print(f"\nğŸ”„ ä½¿ç”¨ Gemini è¿›è¡Œéª†é©¼æ£€æµ‹...")
    
    try:
        # åŠ è½½å›¾ç‰‡å¹¶è½¬æ¢ä¸ºbase64
        image_base64 = load_image_as_base64(TEST_IMAGE)
        
        # æ£€æµ‹æç¤ºè¯
        prompt = """Detect all camels in the image. 
Output a JSON list where each item has:
- "label": the object label (e.g. "camel")
- "box_2d": bounding box as [ymin, xmin, ymax, xmax] normalized to 0-1000

Only output the JSON array, no other text."""

        # ä½¿ç”¨ REST API è°ƒç”¨ Gemini
        contents = [
            {
                "role": "user",
                "parts": [
                    {"inline_data": {"mime_type": "image/jpeg", "data": image_base64}},
                    {"text": prompt}
                ]
            }
        ]
        
        response = gemini_generate_content(
            model=MODELS["gemini"],
            contents=contents,
            response_mime_type="application/json"
        )
        
        result_text = response["candidates"][0]["content"]["parts"][0]["text"]
        print(f"ğŸ“ Gemini åŸå§‹å“åº”: {result_text}")
        
        # è§£æJSON
        detections = json.loads(result_text)
        print(f"âœ… Gemini æ£€æµ‹åˆ° {len(detections)} ä¸ªç›®æ ‡")
        return detections
        
    except Exception as e:
        print(f"âŒ Gemini æ£€æµ‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def test_detection_openai() -> Optional[List[Dict]]:
    """ä½¿ç”¨OpenAI GPT-4Vè¿›è¡Œç›®æ ‡æ£€æµ‹"""
    print(f"\nğŸ”„ ä½¿ç”¨ OpenAI è¿›è¡Œéª†é©¼æ£€æµ‹...")
    
    try:
        client = OpenAI(api_key=API_KEY, base_url=BASE_URLS["openai"])
        
        # åŠ è½½å›¾ç‰‡ä¸ºbase64
        image_base64 = load_image_as_base64(TEST_IMAGE)
        
        # æ£€æµ‹æç¤ºè¯
        prompt = """Detect all camels in the image. 
Output a JSON list where each item has:
- "label": the object label (e.g. "camel")
- "box_2d": bounding box as [ymin, xmin, ymax, xmax] normalized to 0-1000

IMPORTANT: Estimate the bounding box coordinates carefully. The values should be between 0 and 1000.
Only output the JSON array, no other text or markdown formatting."""

        response = client.chat.completions.create(
            model=MODELS["openai"],
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_base64}"
                            },
                        },
                    ],
                }
            ],
            max_tokens=1000,
        )
        
        result_text = response.choices[0].message.content
        print(f"ğŸ“ OpenAI åŸå§‹å“åº”: {result_text}")
        
        # æ¸…ç†å¯èƒ½çš„markdownæ ¼å¼
        if "```json" in result_text:
            result_text = result_text.split("```json")[1].split("```")[0]
        elif "```" in result_text:
            result_text = result_text.split("```")[1].split("```")[0]
        
        result_text = result_text.strip()
        
        # è§£æJSON
        detections = json.loads(result_text)
        print(f"âœ… OpenAI æ£€æµ‹åˆ° {len(detections)} ä¸ªç›®æ ‡")
        return detections
        
    except Exception as e:
        print(f"âŒ OpenAI æ£€æµ‹å¤±è´¥: {e}")
        return None


def fix_json(text: str) -> str:
    """å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é”™è¯¯"""
    import re
    text = text.strip()
    
    # ä¿®å¤ box_2d æ•°ç»„ä¸­ ] å†™æˆ } çš„æƒ…å†µï¼Œå¦‚ [1, 2, 3, 4}} -> [1, 2, 3, 4]}
    # åŒ¹é… box_2d": [æ•°å­—, æ•°å­—, æ•°å­—, æ•°å­—}} çš„æ¨¡å¼
    text = re.sub(r'(\[\s*\d+\s*,\s*\d+\s*,\s*\d+\s*,\s*\d+\s*)\}(\})', r'\1]\2', text)
    
    # ä¿®å¤ç¼ºå°‘ç»“å°¾ ] çš„æƒ…å†µ
    if text.startswith("[") and not text.endswith("]"):
        if text.endswith("}"):
            text = text + "]"
        elif text.endswith("},"):
            text = text[:-1] + "]"
    
    # æ£€æŸ¥æ‹¬å·åŒ¹é…
    open_brackets = text.count("[")
    close_brackets = text.count("]")
    if open_brackets > close_brackets:
        text = text + "]" * (open_brackets - close_brackets)
    
    open_braces = text.count("{")
    close_braces = text.count("}")
    if open_braces > close_braces:
        text = text + "}" * (open_braces - close_braces)
        
    return text


def test_detection_qwen() -> Optional[List[Dict]]:
    """ä½¿ç”¨Qwen-VLè¿›è¡Œç›®æ ‡æ£€æµ‹"""
    print(f"\nğŸ”„ ä½¿ç”¨ Qwen-VL è¿›è¡Œéª†é©¼æ£€æµ‹...")
    
    try:
        client = OpenAI(api_key=API_KEY, base_url=BASE_URLS["qwen"])
        
        # åŠ è½½å›¾ç‰‡ä¸ºbase64
        image_base64 = load_image_as_base64(TEST_IMAGE)
        
        # æ£€æµ‹æç¤ºè¯
        prompt = """Detect all camels in the image. 
Output a JSON list where each item has:
- "label": the object label (e.g. "camel")
- "box_2d": bounding box as [ymin, xmin, ymax, xmax] normalized to 0-1000

IMPORTANT: Estimate the bounding box coordinates carefully. The values should be between 0 and 1000.
Only output the JSON array, no other text or markdown formatting."""

        response = client.chat.completions.create(
            model=MODELS["qwen"],
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_base64}"
                            },
                        },
                    ],
                }
            ],
            max_tokens=1000,
        )
        
        result_text = response.choices[0].message.content
        print(f"ğŸ“ Qwen åŸå§‹å“åº”: {result_text}")
        
        # æ¸…ç†å¯èƒ½çš„markdownæ ¼å¼
        if "```json" in result_text:
            result_text = result_text.split("```json")[1].split("```")[0]
        elif "```" in result_text:
            result_text = result_text.split("```")[1].split("```")[0]
        
        result_text = result_text.strip()
        
        # å°è¯•ä¿®å¤JSONæ ¼å¼
        try:
            detections = json.loads(result_text)
        except json.JSONDecodeError:
            result_text = fix_json(result_text)
            detections = json.loads(result_text)
            
        print(f"âœ… Qwen æ£€æµ‹åˆ° {len(detections)} ä¸ªç›®æ ‡")
        return detections
        
    except Exception as e:
        print(f"âŒ Qwen æ£€æµ‹å¤±è´¥: {e}")
        return None


def test_detection_claude() -> Optional[List[Dict]]:
    """ä½¿ç”¨Claudeè¿›è¡Œç›®æ ‡æ£€æµ‹"""
    print(f"\nğŸ”„ ä½¿ç”¨ Claude è¿›è¡Œéª†é©¼æ£€æµ‹...")
    
    try:
        client = OpenAI(api_key=API_KEY, base_url=BASE_URLS["claude"])
        
        # åŠ è½½å›¾ç‰‡ä¸ºbase64
        image_base64 = load_image_as_base64(TEST_IMAGE)
        
        # æ£€æµ‹æç¤ºè¯
        prompt = """Detect all camels in the image. 
Output a JSON list where each item has:
- "label": the object label (e.g. "camel")
- "box_2d": bounding box as [ymin, xmin, ymax, xmax] normalized to 0-1000

IMPORTANT: Estimate the bounding box coordinates carefully. The values should be between 0 and 1000.
Only output the JSON array, no other text or markdown formatting."""

        response = client.chat.completions.create(
            model=MODELS["claude"],
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_base64}"
                            },
                        },
                    ],
                }
            ],
            max_tokens=1000,
        )
        
        result_text = response.choices[0].message.content
        print(f"ğŸ“ Claude åŸå§‹å“åº”: {result_text}")
        
        # æ¸…ç†å¯èƒ½çš„markdownæ ¼å¼
        if "```json" in result_text:
            result_text = result_text.split("```json")[1].split("```")[0]
        elif "```" in result_text:
            result_text = result_text.split("```")[1].split("```")[0]
        
        result_text = result_text.strip()
        
        # è§£æJSON
        detections = json.loads(result_text)
        print(f"âœ… Claude æ£€æµ‹åˆ° {len(detections)} ä¸ªç›®æ ‡")
        return detections
        
    except Exception as e:
        print(f"âŒ Claude æ£€æµ‹å¤±è´¥: {e}")
        return None


def visualize_detections(
    image_path: Path, 
    detections: List[Dict], 
    output_path: Path,
    model_name: str,
    coord_format: str = "yxyx",  # "yxyx" for [ymin, xmin, ymax, xmax], "xyxy" for [xmin, ymin, xmax, ymax]
    normalized: bool = True  # True if coords are 0-1000, False if pixel coords
) -> None:
    """å¯è§†åŒ–æ£€æµ‹ç»“æœ"""
    image = Image.open(image_path)
    draw = ImageDraw.Draw(image)
    width, height = image.size
    
    # å®šä¹‰é¢œè‰²
    colors = ["#FF6B6B", "#4ECDC4", "#45B7D1", "#96CEB4", "#FFEAA7"]
    
    for i, det in enumerate(detections):
        box = det.get("box_2d", [])
        label = det.get("label", "unknown")
        
        if len(box) != 4:
            continue
        
        # æ ¹æ®åæ ‡æ ¼å¼è½¬æ¢
        if coord_format == "yxyx":
            # Geminiæ ¼å¼: [ymin, xmin, ymax, xmax]
            ymin, xmin, ymax, xmax = box
        else:
            # å…¶ä»–æ¨¡å‹å¯èƒ½ä½¿ç”¨: [xmin, ymin, xmax, ymax]
            xmin, ymin, xmax, ymax = box
        
        # å¦‚æœæ˜¯å½’ä¸€åŒ–åæ ‡ï¼Œè½¬æ¢ä¸ºåƒç´ åæ ‡
        if normalized:
            x1 = int(xmin / 1000 * width)
            y1 = int(ymin / 1000 * height)
            x2 = int(xmax / 1000 * width)
            y2 = int(ymax / 1000 * height)
        else:
            # ç›´æ¥ä½¿ç”¨åƒç´ åæ ‡
            x1, y1, x2, y2 = int(xmin), int(ymin), int(xmax), int(ymax)
        
        color = colors[i % len(colors)]
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        draw.rectangle([x1, y1, x2, y2], outline=color, width=4)
        
        # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
        text = f"{label}"
        try:
            font = ImageFont.truetype("/System/Library/Fonts/Helvetica.ttc", 20)
        except:
            font = ImageFont.load_default()
        
        bbox = draw.textbbox((x1, y1 - 25), text, font=font)
        draw.rectangle([bbox[0] - 2, bbox[1] - 2, bbox[2] + 2, bbox[3] + 2], fill=color)
        draw.text((x1, y1 - 25), text, fill="white", font=font)
    
    # æ·»åŠ æ¨¡å‹åç§°æ°´å°
    try:
        font = ImageFont.truetype("/System/Library/Fonts/Helvetica.ttc", 30)
    except:
        font = ImageFont.load_default()
    draw.text((10, 10), f"Model: {model_name}", fill="#333333", font=font)
    
    # ä¿å­˜
    output_path.parent.mkdir(parents=True, exist_ok=True)
    image.save(output_path)
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {output_path}")


def main():
    print("=" * 60)
    print("ğŸ« MLLM ç›®æ ‡æ£€æµ‹èƒ½åŠ›æµ‹è¯•")
    print("=" * 60)
    
    # Step 1: æµ‹è¯•APIè¿é€šæ€§
    print("\n" + "=" * 60)
    print("ğŸ“¡ ç¬¬ä¸€æ­¥: æµ‹è¯•APIè¿é€šæ€§")
    print("=" * 60)
    
    gemini_ok = test_text_api("gemini")
    openai_ok = test_text_api("openai")
    
    if not gemini_ok and not openai_ok:
        print("\nâŒ é”™è¯¯: Gemini å’Œ OpenAI API éƒ½æ— æ³•è¿æ¥ï¼Œé€€å‡ºæµ‹è¯•")
        sys.exit(1)
    
    # Step 2: æµ‹è¯•ç›®æ ‡æ£€æµ‹
    print("\n" + "=" * 60)
    print("ğŸ¯ ç¬¬äºŒæ­¥: æµ‹è¯•ç›®æ ‡æ£€æµ‹èƒ½åŠ›")
    print("=" * 60)
    
    if not TEST_IMAGE.exists():
        print(f"âŒ é”™è¯¯: æµ‹è¯•å›¾ç‰‡ä¸å­˜åœ¨: {TEST_IMAGE}")
        sys.exit(1)
    
    print(f"ğŸ“· æµ‹è¯•å›¾ç‰‡: {TEST_IMAGE}")
    
    # æµ‹è¯•å„ä¸ªæ¨¡å‹
    results = {}
    
    if gemini_ok:
        detections = test_detection_gemini()
        if detections:
            results["gemini"] = detections
            output_path = OUTPUT_DIR / "gemini_luotuo.jpg"
            # Gemini ä½¿ç”¨ [ymin, xmin, ymax, xmax] æ ¼å¼
            visualize_detections(TEST_IMAGE, detections, output_path, "Gemini", coord_format="yxyx")
    
    if openai_ok:
        detections = test_detection_openai()
        if detections:
            results["openai"] = detections
            output_path = OUTPUT_DIR / "openai_luotuo.jpg"
            visualize_detections(TEST_IMAGE, detections, output_path, "OpenAI GPT-4o", coord_format="yxyx")
    
    # ä¹Ÿæµ‹è¯•å…¶ä»–å‚å•†
    qwen_ok = test_text_api("qwen")
    if qwen_ok:
        detections = test_detection_qwen()
        if detections:
            results["qwen"] = detections
            output_path = OUTPUT_DIR / "qwen_luotuo.jpg"
            # Qwen è¿”å›çš„æ˜¯åƒç´ åæ ‡ [xmin, ymin, xmax, ymax]ï¼Œä¸æ˜¯å½’ä¸€åŒ–åæ ‡
            visualize_detections(TEST_IMAGE, detections, output_path, "Qwen-VL", coord_format="xyxy", normalized=False)
    
    claude_ok = test_text_api("claude")
    if claude_ok:
        detections = test_detection_claude()
        if detections:
            results["claude"] = detections
            output_path = OUTPUT_DIR / "claude_luotuo.jpg"
            visualize_detections(TEST_IMAGE, detections, output_path, "Claude", coord_format="yxyx")
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    for model, dets in results.items():
        print(f"\n{model}:")
        for det in dets:
            print(f"  - {det.get('label', 'unknown')}: {det.get('box_2d', [])}")
    
    if not results:
        print("âš ï¸ æ²¡æœ‰ä»»ä½•æ¨¡å‹æˆåŠŸæ£€æµ‹åˆ°ç›®æ ‡")
    else:
        print(f"\nâœ… æˆåŠŸå®Œæˆ! ç»“æœä¿å­˜åœ¨: {OUTPUT_DIR}")


if __name__ == "__main__":
    main()

